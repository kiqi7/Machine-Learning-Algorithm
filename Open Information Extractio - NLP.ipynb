{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Information Extraction\n",
    "\n",
    "Information extraction takes a body of freeform text and extracts the contained information in a computer interpretable form. The word *open* simply means that the text/facts are arbitrary, so it will work with any input rather than a specific domain (e.g. legal texts).\n",
    "\n",
    "As an example, given the input:\n",
    "\n",
    "> \"Trolls really don't like the sun.\"\n",
    "  \n",
    "\n",
    "you may extract the \"fact\":\n",
    "```\n",
    "('Trolls', 'do not like', 'the sun')\n",
    "```\n",
    "\n",
    "The approach is based on the paper \"*Identifying Relations for Open Information Extraction*\", by Fader, Soderland & Etzioni. Parts have been updated with more recent, or more ML, techniques however (Q3 and Q4 match the paper; Q1 and Q2 don't). You don't have to read the paper as this workbook takes you through the process; this lab is also intended as a tutorial/opportunity to see one way such a system works. Some parts are rule based, as that's still often the case. Note that the several parts of a complete system have been dropped, as the lab would be too much work otherwise; consequentially it isn't going to work that well.\n",
    "\n",
    "The steps of the system are as follows:\n",
    "*  Tokenise and split on sentences *(provided)*\n",
    "\n",
    "\n",
    "1. Part of speech tagging - token level\n",
    "2. Part of speech tagging - sentence level\n",
    "3. Named entity resolution\n",
    "4. Relation extraction\n",
    "\n",
    "\n",
    "*  Summarise \"*20,000 leagues under the seas*\" by Jules Verne *(provided)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ogonek\n",
    "\n",
    "A tiny NLP library\n",
    "\n",
    "### Tokenisation and sentence splitting\n",
    "`ogonek.Tokenise()`\n",
    "\n",
    "A class that tokenises some text and splits it into sentences. Construct an instance with `tokens = ogonek.Tokenise('My text')`; it then has the same interface as a list of lists:\n",
    "* `len(tokens)`: Number of extracted sentences (not words)\n",
    "* `tokens[i]`: Sentence i, where i ranges from 0 to one less than `len(tokens)`. A sentence is a list of tokens.\n",
    "\n",
    "\n",
    "\n",
    "### Word vectors\n",
    "`ogonek.Glove()`\n",
    "\n",
    "Constructing a `glove = ogonek.Glove()` object loads a heavily pruned Glove word vectors from the file `baby_glove.zip` into memory, and will then translate tokens into word vectors. Note that it automatically lowercases any token it is handed, so you don't need to. Has the following interface:\n",
    "* `glove.len_vec()` - Returns the length of the word vectors; should be 300.\n",
    "* `len(glove)` - Returns how many word vectors it knows of.\n",
    "* `token in glove` - Returns `True` if it has a word vector for that token, `False` otherwise.\n",
    "* `glove[token]` - Returns the word vector for the given token; raises an error if it does not have one.\n",
    "* `glove.decode(token)` - Returns the word vector for the given token, but if the word vector is unknown returns a vector of zeros instead (silent failure).\n",
    "* `glove.decodes(list of tokens)` - Returns a list of word vectors, one for each token. Has the same silent failure behaviour as `decode`.\n",
    "\n",
    "\n",
    "\n",
    "### Groningen Meaning Bank dataset\n",
    "`ogonek.GMB()`\n",
    "\n",
    "Provides access to the Groningen Meaning Bank dataset, which is supplied in the file `ner_dataset.csv`. Replicates the interface of the tokenisation system as far as it can. Construct with `gmb = ogonek.GMB()`; has the following interface:\n",
    "* `len(gmb)`: Number of sentences (not words) in data set\n",
    "* `gmb[i]`: Sentence i, where i ranges from 0 to one less than `len(gmb)`. A sentence is a list of tokens.\n",
    "* `gmb.pos(i)`: A list of POS tags that match with sentence i. Note that these are the full Penn Treebank tags (not the reduced set used below).\n",
    "* `gmb.ner(i)`: A list of named entities that match with sentence i. Using outside-inside scheme.\n",
    "\n",
    "\n",
    "\n",
    "### Pretty printing\n",
    "\n",
    "`ogonek.aligned_print(*)` takes multiple lists and prints them out, aligning them so that all elements in position 0 of all lists are aligned vertically (extra space added as required), and then elements in position 1 and so on. For showing tags and a sentence with everything aligned. Also does word wrap and colour coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ogonek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary giving descriptions of the reduced part of speech tags...\n",
    "rpos_desc = {'C' : 'Coordinating conjunction',\n",
    "             '0' : 'Cardinal number',\n",
    "             'D' : 'Determiner',\n",
    "             'E' : 'Existential there',\n",
    "             'I' : 'Preposition or subordinating conjunction',\n",
    "             'J' : 'Adjective',\n",
    "             'N' : 'Noun',\n",
    "             'P' : 'Predeterminer',\n",
    "             'S' : 'Possessive ending',\n",
    "             'M' : 'Pronoun',\n",
    "             'R' : 'Adverb',\n",
    "             'Z' : 'Particle',\n",
    "             'T' : 'to',\n",
    "             'V' : 'Verb',\n",
    "             'A' : 'Anything else',\n",
    "             '.' : 'All punctuation'}\n",
    "\n",
    "\n",
    "# Reduced list of part of speech tags as a list...\n",
    "num_to_rpos = ['C', '0', 'D', 'E', 'I', 'J', 'N', 'P',\n",
    "               'S', 'M', 'R', 'Z', 'T', 'V', 'A', '.']\n",
    "\n",
    "\n",
    "# Dictionary that maps a reduced part of speech\n",
    "# tag to it's index in the above list; useful for vectors/matrices etc...\n",
    "rpos_to_num = {'C' : 0,\n",
    "               '0' : 1,\n",
    "               'D' : 2,\n",
    "               'E' : 3,\n",
    "               'I' : 4,\n",
    "               'J' : 5,\n",
    "               'N' : 6,\n",
    "               'P' : 7,\n",
    "               'S' : 8,\n",
    "               'M' : 9,\n",
    "               'R' : 10,\n",
    "               'Z' : 11,\n",
    "               'T' : 12,\n",
    "               'V' : 13,\n",
    "               'A' : 14,\n",
    "               '.' : 15}\n",
    "\n",
    "\n",
    "# Dictionary that maps the full part of speech tags to the reduced set...\n",
    "pos_to_rpos = {'CC' : 'C',\n",
    "               'CD' : '0',\n",
    "               'DT' : 'D',\n",
    "               'EX' : 'E',\n",
    "               'FW' : 'A',\n",
    "               'IN' : 'I',\n",
    "               'JJ' : 'J',\n",
    "               'JJR' : 'J',\n",
    "               'JJS' : 'J',\n",
    "               'LS' : 'A',\n",
    "               'MD' : 'A',\n",
    "               'NN' : 'N',\n",
    "               'NNS' : 'N',\n",
    "               'NNP' : 'N',\n",
    "               'NNPS' : 'N',\n",
    "               'PDT' : 'P',\n",
    "               'POS' : 'S',\n",
    "               'PRP' : 'M',\n",
    "               'PRP$' : 'M',\n",
    "               'RB' : 'R',\n",
    "               'RBR' : 'R',\n",
    "               'RBS' : 'R',\n",
    "               'RP' : 'Z',\n",
    "               'SYM' : 'A',\n",
    "               'TO' : 'T',\n",
    "               'UH' : 'A',\n",
    "               'VB' : 'V',\n",
    "               'VBD' : 'V',\n",
    "               'VBG' : 'V',\n",
    "               'VBN' : 'V',\n",
    "               'VBP' : 'V',\n",
    "               'VBZ' : 'V',\n",
    "               'WDT' : 'D',\n",
    "               'WP' : 'M',\n",
    "               'WP$' : 'S',\n",
    "               'WRB' : 'R',\n",
    "               '-' : '.',\n",
    "               'LRB' : '.',\n",
    "               'RRB' : '.',\n",
    "               '``' : '.',\n",
    "               '\"' : '.',\n",
    "               '.' : '.',\n",
    "               ',' : '.',\n",
    "               ';' : '.',\n",
    "               ':' : '.',\n",
    "               '$' : '.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load book, tokenise and split on sentences\n",
    "The below code reads in the book, chops it down to just the text of the book, and then tokenises it using the provided `ogonek` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. It was the regime of verticality .\n",
      "02. Now then , the tides are not strong in the Pacific , and if you can not unballast the Nautilus , which seems impossible to me , I do not see how it will float off . \"\n",
      "03. Captain Nemo left the cave , and we climbed back up the bank of shellfish in the midst of these clear waters not yet disturbed by divers at work .\n",
      "04. Likewise the pilothouse and the beacon housing were withdrawn into the hull until they lay exactly flush with it .\n",
      "05. Instead of digging all around the Nautilus , which would have entailed even greater difficulties , Captain Nemo had an immense trench outlined on the ice , eight meters from our port quarter .\n",
      "06. We would not go five miles without bumping into a fellow countryman .\n",
      "07. The oars , mast , and sail are in the skiff .\n",
      "08. Under existing conditions some ten men at the most should be enough to operate it . \"\n",
      "09. Nobody appeared on our arrival .\n",
      "10. We gasped .\n"
     ]
    }
   ],
   "source": [
    "# Loop file, only keeping lines between indicators...\n",
    "lines = []\n",
    "record = False\n",
    "\n",
    "with open('20,000 Leagues Under the Seas.txt', 'r', encoding='utf8') as fin:\n",
    "    for line in fin:\n",
    "        if record:\n",
    "            if line.startswith('***END OF THE PROJECT GUTENBERG'):\n",
    "                break\n",
    "            lines.append(line)\n",
    "    \n",
    "        else:\n",
    "            if line.startswith('***START OF THE PROJECT GUTENBERG'):\n",
    "                record = True\n",
    "\n",
    "text = ''.join(lines)\n",
    "\n",
    "# Tokenise...\n",
    "under_the_seas = ogonek.Tokenise(text)\n",
    "\n",
    "# Print 10 random sentences to check it worked...\n",
    "numpy.random.seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "    toks = numpy.random.choice(under_the_seas)\n",
    "    print('{:02d}. {}'.format(i+1, ' '.join(toks)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Part of speech tagging - token level\n",
    "\n",
    "The goal here is to train a classifier that indicates which of the part of speech tags (the reduced set provided above) each word is. For this initial approach you're going to treat words (tokens) individually, without context. For features the Glove word vectors are going to be used (provided by `ogonek.Glove()`).\n",
    "\n",
    "Instead of training a single classifier a slight modification of a random kitchen sink for each part of speech tag is going to be used. Specifically, a logistic random kitchen sink that indicates the probability that the word should be labelled with the associated tag. This is a *one vs all* classifier - you have a classifier for every tag, run them all on each word, and then select the tag with the highest probability (it's inconsistent - they won't sum to 1!). A logistic random kitchen sink is simply a normal kitchen sink that is pushed through a sigmoid function (in neural network terms, the final layer has a non-linearity),\n",
    "$$\\operatorname{Sig}(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "such that the final binary classifier is\n",
    "$$P(\\textrm{tag}) = \\operatorname{Sig}\\left(\\sum_{k \\in K} \\alpha_k \\phi\\left(\\vec{x} \\cdot \\vec{w}_k\\right)\\right)$$\n",
    "For the cost function you should maximise the log likelihood of the dataset. This will require gradient descent; Nestorov or better, including backtracking line search to select the initial step size. You can either differentiate yourself, copy the equations from lecture 7 of ML1 (where they were derived), or use tensor flow; your choice! The non-linearity, $\\phi(\\cdot)$ is up to you ($\\sin$ works). Remember to include the original features plus the value `1` when creating the extended feature vector (so it has a bias term). It is suggested to use 300 random features, in addition to the 300 provided by glove (total of 601 - bias term is the +1), as that keeps the resulting data matrix during training small enough that it completes reasonably quickly.\n",
    "\n",
    "The Groningen Meaning Bank dataset has been provided; it can be accessed via the class `ogonek.GMB`. It includes lots of sentences, each as a list of tokens, plus part of speech tags as a list aligned with the sentence.\n",
    "Source: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      "[[0.79172504 0.52889492 0.56804456 0.92559664 0.07103606 0.0871293\n",
      "  0.0202184  0.83261985 0.77815675 0.87001215]]\n",
      "result: \n",
      "[[1 1 1 1 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "a = (numpy.random.rand(1,10)) #create a random (1,10) array\n",
    "print (\"a: \\n\" + str(a))\n",
    "result = (numpy.where(a>0.5,1,0)) \n",
    "print (\"result: \\n\" +str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMB sentences = 47959\n",
      "\n",
      "\u001b[0mThe U.S.  space agency is  making final preparations to launch the first \n",
      "\u001b[31mDT  NNP   NN    NN     VBZ VBG    JJ    NNS          TO VB     DT  JJ    \n",
      "\u001b[34mO   B-geo O     O      O   O      O     O            O  O      O   O     \n",
      "\u001b[0m\n",
      "\u001b[0mdirect space probe to the distant planet of Pluto . \n",
      "\u001b[31mJJ     NN    NN    TO DT  JJ      NN     IN NNP   . \n",
      "\u001b[34mO      O     O     O  O   O       O      O  B-geo O \n",
      "\u001b[0m\n",
      "\u001b[0mOn Monday , the freighter Torgelow was hijacked off the eastern coast of \n",
      "\u001b[31mIN NNP    , DT  NN        NNP      VBD VBN      IN  DT  JJ      NN    IN \n",
      "\u001b[34mO  B-tim  O O   O         B-art    O   O        O   O   O       O     O  \n",
      "\u001b[0m\n",
      "\u001b[0mSomalia . \n",
      "\u001b[31mNNP     . \n",
      "\u001b[34mB-geo   O \n",
      "\u001b[0m\n",
      "\u001b[0mChile and Bolivia are associate members . \n",
      "\u001b[31mNNP   CC  NNP     VBP JJ        NNS     . \n",
      "\u001b[34mB-gpe O   B-gpe   O   O         O       O \n",
      "\u001b[0m\n",
      "\u001b[0mVenezuela has freed 11 Colombian soldiers who had been detained after entering \n",
      "\u001b[31mNNP       VBZ VBN   CD JJ        NNS      WP  VBD VBN  VBN      IN    VBG      \n",
      "\u001b[34mB-geo     O   O     O  B-gpe     O        O   O   O    O        O     O        \n",
      "\u001b[0m\n",
      "\u001b[0mVenezuelan territory without authorization . \n",
      "\u001b[31mJJ         NN        IN      NN            . \n",
      "\u001b[34mB-gpe      O         O       O             O \n",
      "\u001b[0m\n",
      "\u001b[0mHowever , the closing figure of 12,012 points was below the record level of \n",
      "\u001b[31mRB      , DT  NN      NN     IN CD     NNS    VBD IN    DT  NN     NN    IN \n",
      "\u001b[34mO       O O   O       O      O  O      O      O   O     O   O      O     O  \n",
      "\u001b[0m\n",
      "\u001b[0m12,049 points reached during trading Wednesday . \n",
      "\u001b[31mCD     NNS    VBN     IN     NN      NNP       . \n",
      "\u001b[34mO      O      O       O      O       B-tim     O \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load word vectors; in a seperate cell as this takes a couple seconds...\n",
    "glove = ogonek.Glove()\n",
    "\n",
    "# Groningen Meaning Bank dataset - a set of sentences each tagged\n",
    "# with part of speech and named entity recognitiuon tags...\n",
    "gmb = ogonek.GMB()\n",
    "print('GMB sentences = {}'.format(len(gmb)))\n",
    "print()\n",
    "\n",
    "# Print out 5 random sentences from GMB with POS and NER tags, to illustrate the data...\n",
    "numpy.random.seed(1)\n",
    "for _ in range(5):\n",
    "    i = numpy.random.randint(len(gmb))\n",
    "    ogonek.aligned_print(gmb[i], gmb.pos(i), gmb.ner(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 14387 sentences for training\n",
      "Training C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_eta 0.40960000000000013\n",
      "loglike -2676.2416434100764\n",
      "  (took 79.2629 seconds)\n",
      "Training 0\n",
      "best_eta 0.40960000000000013\n",
      "loglike -5811.897890510623\n",
      "  (took 47.7612 seconds)\n",
      "Training D\n",
      "best_eta 0.0001063382396627935\n",
      "loglike -54965.52249305237\n",
      "  (took 39.9793 seconds)\n",
      "Training E\n",
      "best_eta 0.40960000000000013\n",
      "loglike -3663.8971543855987\n",
      "  (took 70.2357 seconds)\n",
      "Training I\n",
      "best_eta 1.1417981541647708e-05\n",
      "loglike -221703.79048375427\n",
      "  (took 50.9858 seconds)\n",
      "Training J\n",
      "best_eta 0.13421772800000006\n",
      "loglike -85641.73545240676\n",
      "  (took 43.2894 seconds)\n",
      "Training N\n",
      "best_eta 1.2259964326927154e-06\n",
      "loglike -223552.12578553427\n",
      "  (took 49.1793 seconds)\n",
      "Training P\n",
      "best_eta 0.5120000000000001\n",
      "loglike -3387.626963236436\n",
      "  (took 95.9931 seconds)\n",
      "Training S\n",
      "best_eta 0.40960000000000013\n",
      "loglike -2558.8163897880495\n",
      "  (took 73.6141 seconds)\n",
      "Training M\n",
      "best_eta 0.40960000000000013\n",
      "loglike -38612.076334565725\n",
      "  (took 33.7343 seconds)\n",
      "Training R\n",
      "best_eta 0.32768000000000014\n",
      "loglike -56265.74154886884\n",
      "  (took 33.1726 seconds)\n",
      "Training Z\n",
      "best_eta 0.40960000000000013\n",
      "loglike -5508.000680257618\n",
      "  (took 45.269 seconds)\n",
      "Training T\n",
      "best_eta 0.40960000000000013\n",
      "loglike -3277.1188867776823\n",
      "  (took 81.3762 seconds)\n",
      "Training V\n",
      "best_eta 1.9156194260823675e-06\n",
      "loglike -202256.77038008565\n",
      "  (took 63.1259 seconds)\n",
      "Training A\n",
      "best_eta 0.40960000000000013\n",
      "loglike -5597.788492377129\n",
      "  (took 57.9938 seconds)\n",
      "Training .\n",
      "best_eta 0.08589934592000005\n",
      "loglike -30724.562377306887\n",
      "  (took 39.8122 seconds)\n"
     ]
    }
   ],
   "source": [
    "# A test/train split - train with [0:split], test with [split:len(gmb)]\n",
    "split = int(len(gmb) * 0.3) # Have a lot of data, and don't want you waiting around too long to train!\n",
    "print('Using {} sentences for training'.format(split))\n",
    "\n",
    "x_train = gmb[0:split]\n",
    "x_test = gmb[split:len(gmb)]\n",
    "\n",
    "con_train = numpy.concatenate(x_train)\n",
    "con_test = numpy.concatenate(x_test)\n",
    "train_decode = glove.decodes(con_train)\n",
    "test_decode = glove.decodes(con_test)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + numpy.exp(-z))\n",
    "\n",
    "# update alpha gradient to get new alpha\n",
    "def Nesterov_gradient(y,ex,lamb_da, step_size,alpha_grad,initial_alpha):\n",
    "    update_gradient = numpy.random.normal(0, 1, 601)\n",
    "    alpha = initial_alpha\n",
    "    loglike = loss_fun(y,ex,alpha)\n",
    "    j = 0\n",
    "    \n",
    "    for i in range (256):\n",
    "        \n",
    "        alpha_grad = alpha_gradient(y,ex,alpha+lamb_da* update_gradient)\n",
    "        \n",
    "        update_gradient = lamb_da * update_gradient + step_size * alpha_grad\n",
    "        alpha = alpha + update_gradient\n",
    "        \n",
    "        new_loglike = loss_fun(y,ex,alpha)\n",
    "        # if gradient stop update, after 8 times break of loop\n",
    "        if new_loglike > loglike:\n",
    "            loglike = new_loglike\n",
    "\n",
    "        else:\n",
    "            j += 1\n",
    "            if j>8:\n",
    "                break \n",
    "        \n",
    "    return alpha,loglike\n",
    "\n",
    "# from the log likelihood to get the alpha gradient\n",
    "def alpha_gradient(y,ex,alpha):    \n",
    "    alpha_grad = numpy.zeros(601)\n",
    "\n",
    "    p = sigmoid(ex.dot(alpha))\n",
    "    alpha_grad += (y - p)@ ex\n",
    "\n",
    "    return alpha_grad\n",
    "\n",
    "# Armijo_goldstein which is backtracking line search,this is to get the best step size\n",
    "# only have alpha as variable\n",
    "def Armijo_goldstein(x,y,ex,eta,alpha,alpha_grad,w):\n",
    "    mu = 0.5\n",
    "\n",
    "    while (loss_fun(y,ex,alpha + eta * alpha_grad) < loss_fun(y,ex,alpha) + eta * mu * numpy.linalg.norm(alpha_gradient(y,ex,alpha), ord = 2)):       \n",
    "        eta =  eta * 0.8\n",
    "        if eta < 1e-6:\n",
    "            break\n",
    "    return eta\n",
    "\n",
    "# loss function is calculated the log likelihood\n",
    "# loss function only have alpha as variable\n",
    "def loss_fun(y,ex,alpha):\n",
    "\n",
    "    q = sigmoid(ex.dot(alpha))\n",
    "    q = numpy.clip(q, 1e-3, 1 - 1e-3)\n",
    "    p = y\n",
    "    \n",
    "    L =sum(p*numpy.log(q) + (1-p)*numpy.log(1-q))\n",
    "    # maximum log likelihood value for calculation\n",
    "    \n",
    "    return L\n",
    "\n",
    "def train(x, w):\n",
    "\n",
    "    nf = numpy.sin(numpy.einsum('ef,gf->eg', x, w)) \n",
    "    ex = numpy.append(x, nf, axis=1)\n",
    "    bias = numpy.ones((x.shape[0],1))\n",
    "    ex = numpy.append(ex, bias, axis=1)\n",
    "    return nf, ex\n",
    "\n",
    "x_train = numpy.array(train_decode)\n",
    "x_test = numpy.array(test_decode)\n",
    "\n",
    "def train_tag_model(tag):   \n",
    "    start = time.time()\n",
    "\n",
    "    lamb_da = 0.8\n",
    "    \n",
    "    y_train = (numpy.array([pos_to_rpos[tag] for i in range(split) for tag in gmb.pos(i)]) == tag).astype(int)\n",
    "\n",
    "    w = numpy.random.standard_normal((300, 300))\n",
    "    # alpha size is 601\n",
    "    initial_alpha = numpy.random.normal(0, 1, 601)\n",
    "    initial_alpha /= numpy.linalg.norm(initial_alpha)\n",
    "    \n",
    "    # nf, ex\n",
    "    nf, ex = train(x_train,w)\n",
    "    # alpha gradient\n",
    "    alpha_grad =  alpha_gradient(y_train, ex,initial_alpha)\n",
    "   \n",
    "    # best step size: backtracking\n",
    "    eta = 1\n",
    "    best_eta = Armijo_goldstein(x_train,y_train,ex,eta,initial_alpha,alpha_grad,w)    \n",
    "    print('best_eta',best_eta)\n",
    "    # nestrov\n",
    "#     best_eta = 1\n",
    "    new_alpha,loglike = Nesterov_gradient(y_train,ex,lamb_da, best_eta, alpha_grad,initial_alpha)  \n",
    "    print('loglike',loglike)\n",
    "    end = time.time()\n",
    "    print('  (took {:g} seconds)'.format(end-start))\n",
    "    return new_alpha,w\n",
    "\n",
    "# Code to train a model for each reduced POS tag...\n",
    "rpos_model = {}\n",
    "for tag in rpos_desc:\n",
    "    print('Training {}'.format(tag))\n",
    "    rpos_model[tag] = train_tag_model(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage correct = 63.4%\n",
      "  (took 330.334 seconds)\n"
     ]
    }
   ],
   "source": [
    "def token_pos(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this should return part of\n",
    "    speech tags, as a list of strings (the codes in the rpos_desc dictionary).\n",
    "    Basically calls the models for each tag and selects the tag with the\n",
    "    highest probability.\"\"\"\n",
    "    \n",
    "    x = glove.decodes(sentence)\n",
    "    x = numpy.array(x)\n",
    "    \n",
    "    z_value = []\n",
    "    predict_tag = []\n",
    "    final_pred = []\n",
    "    tag_pred = ['i'] * len(sentence)\n",
    "    for tag in rpos_desc:\n",
    "        update_alpha,weight = rpos_model[tag]\n",
    "\n",
    "        nf = numpy.sin(numpy.einsum('ef,gf->eg', x, weight))\n",
    "        ex = numpy.append(x, nf, axis=1)\n",
    "        bias = numpy.ones((x.shape[0],1))\n",
    "        ex = numpy.append(ex, bias, axis=1)\n",
    "\n",
    "        pred_y = sigmoid(ex.dot(update_alpha)) # calculate the prediction value\n",
    "\n",
    "        predict_tag.append(pred_y)\n",
    "    \n",
    "    kitc_sink_p = numpy.zeros(len(sentence))\n",
    "    for j in range(len(sentence)):\n",
    "        word_list = []\n",
    "\n",
    "        for i in range (16):\n",
    "            word_list.append(predict_tag[i][j]) \n",
    "        # find the maximum probability tag index\n",
    "        index = numpy.argmax(word_list)\n",
    "        \n",
    "        kitc_sink_p[j] = max(word_list)\n",
    "        tag_pred[j] = num_to_rpos[index]        \n",
    " \n",
    "    return tag_pred,kitc_sink_p\n",
    "                \n",
    "#     # maximum 16 probabilities\n",
    "# Code to test the performance of your POS tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100 \n",
    "\n",
    "start = time.time()\n",
    "# for i in range(split, len(gmb)):\n",
    "for i in range(0, split):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess,senten_prob = token_pos(gmb[i])\n",
    "    truth = gmb.pos(i)\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==pos_to_rpos[t]:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part of speech tagging - sentence level\n",
    "\n",
    "While the previous step works very well you need POS tags to be super accurate, as everything else depends on them. You will now introduce context. This is done by calculating transition probabilities between tags and solving a Markov random chain using the forward-backwards algorithm (or just forward if you keep links; it's dynamic programming) to find the maximum a posteriori (MAP) POS tag assignment for the entire sentence. The adjacency matrix should contain $\\log P(\\textrm{second pos tag} | \\textrm{first pos tag})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.90775528, -3.78199589, -6.73195975, -6.90775528, -5.56552486,\n",
       "        -2.78884616, -0.42635699, -6.90775528, -6.90775528, -5.15461015,\n",
       "        -5.00356989, -6.90775528, -6.90775528, -3.08884871, -6.90775528,\n",
       "        -1.61807142],\n",
       "       [-3.28380429, -2.89882847, -2.43762138, -6.90775528, -1.16104776,\n",
       "        -2.45995733, -2.38545831, -6.90775528, -5.02833487, -5.557179  ,\n",
       "        -3.48153253, -6.34563636, -3.79359041, -1.76475887, -6.90775528,\n",
       "        -2.37912517],\n",
       "       [-3.50465757, -5.85358084, -6.17417973, -6.90775528, -0.76944728,\n",
       "        -6.13556489, -3.13341387, -6.38363783, -6.90775528, -5.9749417 ,\n",
       "        -3.95703245, -5.08083061, -3.34421389, -1.11098685, -6.90775528,\n",
       "        -2.78486367],\n",
       "       [-2.12963129, -6.90775528, -4.96284463, -6.90775528, -1.8947917 ,\n",
       "        -4.96284463, -2.47793798, -6.90775528, -6.90775528, -6.90775528,\n",
       "        -3.35340672, -6.90775528, -6.90775528, -0.63211129, -6.90775528,\n",
       "        -2.71155283],\n",
       "       [-4.9251881 , -3.98735552, -5.45917205, -6.90775528, -4.29512879,\n",
       "        -3.33571363, -0.45179922, -6.90775528, -6.90775528, -5.14749448,\n",
       "        -3.6803531 , -5.24867962, -6.90775528, -1.53200713, -6.90775528,\n",
       "        -3.47208079],\n",
       "       [-3.08898802, -3.43889919, -1.13983002, -6.90775528, -1.73450568,\n",
       "        -2.38103153, -3.08820309, -6.90775528, -3.03065244, -3.53962793,\n",
       "        -3.47636101, -6.07184709, -4.47646093, -1.97737334, -6.90775528,\n",
       "        -3.59082394],\n",
       "       [-3.73970342, -3.28491609, -1.62169895, -6.90775528, -2.0415732 ,\n",
       "        -1.6464321 , -1.32741388, -6.90775528, -3.86966785, -3.94430432,\n",
       "        -5.52456814, -6.54199997, -4.75954289, -2.62828249, -6.90775528,\n",
       "        -3.60948156],\n",
       "       [-3.93182563, -6.90775528, -4.62497281, -6.90775528, -0.8873032 ,\n",
       "        -4.62497281, -4.62497281, -6.90775528, -6.90775528, -4.62497281,\n",
       "        -3.0155349 , -3.93182563, -3.93182563, -0.84078318, -6.90775528,\n",
       "        -4.62497281],\n",
       "       [-6.90775528, -6.90775528, -6.90775528, -6.90775528, -6.90775528,\n",
       "        -6.90775528, -0.00841766, -6.90775528, -6.90775528, -6.90775528,\n",
       "        -6.90775528, -6.90775528, -6.90775528, -6.90775528, -6.90775528,\n",
       "        -5.07429843],\n",
       "       [-2.96974125, -6.30985094, -4.31742077, -6.90775528, -1.22867664,\n",
       "        -5.67386217, -2.50566064, -6.90775528, -6.90775528, -3.9602409 ,\n",
       "        -3.36541196, -4.8961576 , -4.08823491, -0.90817506, -6.90775528,\n",
       "        -2.60535334],\n",
       "       [-3.38950134, -5.03043792, -3.37985807, -6.90775528, -3.10618927,\n",
       "        -4.70835442, -1.38943655, -6.90775528, -5.27889928, -3.36084609,\n",
       "        -2.8775136 , -5.87440799, -4.5857521 , -0.9909016 , -2.96484179,\n",
       "        -2.40287497],\n",
       "       [-6.90775528, -6.90775528, -6.3744568 , -6.90775528, -6.77992191,\n",
       "        -6.90775528, -5.68130962, -6.90775528, -6.90775528, -3.18260965,\n",
       "        -6.77992191, -6.90775528, -6.90775528, -0.05189156, -6.90775528,\n",
       "        -6.77992191],\n",
       "       [-5.03963382, -4.19096328, -6.35182021, -6.90775528, -4.72819766,\n",
       "        -2.95300662, -0.74864765, -6.90775528, -6.90775528, -4.35614855,\n",
       "        -3.47342464, -4.73526483, -6.90775528, -0.98251197, -6.90775528,\n",
       "        -4.18685649],\n",
       "       [-3.39478807, -5.45226263, -3.60114164, -5.58273311, -3.43222261,\n",
       "        -5.82500714, -0.9110932 , -6.90775528, -6.90775528, -2.6437135 ,\n",
       "        -2.87370539, -6.90775528, -2.35616082, -1.69181137, -3.29716236,\n",
       "        -3.01253144],\n",
       "       [-3.48977353, -5.4491974 , -2.73797762, -5.19788297, -6.90775528,\n",
       "        -6.54780969, -0.47608268, -6.90775528, -6.90775528, -1.52581063,\n",
       "        -4.27421213, -6.90775528, -6.90775528, -4.68705735, -6.90775528,\n",
       "        -3.46983931],\n",
       "       [-6.36664874, -3.0174069 , -4.87522974, -6.90775528, -4.59554131,\n",
       "        -3.72087547, -0.22595124, -6.90775528, -6.90775528, -5.01149462,\n",
       "        -3.48575572, -6.2679139 , -6.42491765, -2.97144412, -6.90775528,\n",
       "        -4.12935717]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjacency probabilities\n",
    "prob_matrix = numpy.zeros((16,16))\n",
    "\n",
    "for i in range (split, len(gmb)):\n",
    "    sent = gmb.pos(i)\n",
    "    j = 0\n",
    "    while j < len(sent)-1:\n",
    "        row = rpos_to_num[pos_to_rpos[sent[j]]]\n",
    "        column = rpos_to_num[pos_to_rpos[sent[j+1]]]\n",
    "        j += 1\n",
    "        prob_matrix[column][row] +=1\n",
    "        \n",
    "# normalize prob_matrix row value    \n",
    "row_sum = numpy.sum(prob_matrix,axis = 1)\n",
    "norm_matrix = prob_matrix / row_sum[:,numpy.newaxis]\n",
    "norm_matrix = numpy.clip(norm_matrix, 1e-3, 1 - 1e-3)\n",
    "\n",
    "numpy.log(norm_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x,label,start_p,adjacency_p,emit_p):\n",
    "    v= [{}]\n",
    "    path = {}\n",
    "    #initial t = 0,\n",
    "    for y in label:\n",
    "        v[0][y] = start_p[y] * emit_p[y][x[0]]\n",
    "        path[y] = y\n",
    "\n",
    "    for t in range (1,len(x)):\n",
    "        v.append({})\n",
    "        newpath= {}\n",
    "        for tag in label:\n",
    "            (prob,gues_label) = max([(v[t-1][y_0] * adjacency_p[y_0][tag]*emit_p[tag][x[t]],y_0) for y_o in label])\n",
    "            v[t][tag] = prob\n",
    "            newpath[tag] = path[gues_label] + [tag]\n",
    "        path = newpath\n",
    "\n",
    "    (prob,gues_label) =max([(v[len(x)-1][y],y) for y in label])\n",
    "    return (prob,path[gues_label])\n",
    "\n",
    "def emission_prob(sentence): \n",
    "    count_tag = numpy.zeros(16)\n",
    "    emi = numpy.zeros(16,len(sentence))\n",
    "    for i in sentence:\n",
    "        # count the numbers of different tag appear in sentence\n",
    "        for x,v in enumerate(num_to_rpos): \n",
    "            if pos_to_rpos[i] == v:\n",
    "                count_tag[x] +=1 \n",
    "    # count the numbers of each words appear in sentence\n",
    "    count_word = [sentence.count(x) for x in sentence]\n",
    "    \n",
    "    # each word frequency to divide the each tag frequency \n",
    "    # calculate the probability of observing words from state tag\n",
    "    for i in range (16):\n",
    "        for j in range(len(sentence)):\n",
    "            emi[i][j] = count_word[j]/count_tag[i]\n",
    "    return count_v\n",
    "\n",
    "\n",
    "def sentence_pos(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this should return part of\n",
    "    speech tags, as a list of strings (the codes in the rpos_desc dictionary).\n",
    "    A more advanced version of token_pos that uses neighbours as well.\"\"\"\n",
    "    x = glove.decodes(sentence)\n",
    "    x = numpy.array(x)\n",
    "    \n",
    "    # transition probability\n",
    "    adjacency_p = numpy.log(norm_matrix)\n",
    "    kitche_tag,kitsink_p = token_pos(sentence)\n",
    "    \n",
    "    #normalize random sink kitchen probability\n",
    "    start_p = [float(i)/sum(kitsink_p) for i in len(kitsink_p)]\n",
    "    \n",
    "    label = len(rpos_to_num.values())\n",
    "    \n",
    "    emit_p = emission_prob(sentence)\n",
    "    \n",
    "    #calculation the prediction\n",
    "    pred_y = viterbi(x,label,start_p,adjacency_p,emit_p)\n",
    "        \n",
    "    return pred_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test the performance of your improved POS tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100 # If you want faster feedback you can reduce this\n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = sentence_pos(gmb[i])\n",
    "    truth = gmb.pos(i)\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==pos_to_rpos[t]:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named entity recognition\n",
    "\n",
    "The next step is to identify names, that is the entities that \"facts\" may apply to. While training a further classifier does work (same as above, inc. dynamic programming) there would be little point in repeating the exercise. Instead, a simple rule based approach using *regular expressions* is going to be used.\n",
    "\n",
    "You will probably want to look at the Python 3 documentation:\n",
    "https://docs.python.org/3/library/re.html\n",
    "There is also the how to, a tutorial:\n",
    "https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "\n",
    "Given part of speech tagging a name can be defined as:\n",
    "* An optional *determiner*, e.g. *the* (1 or none)\n",
    "* An arbitrary number of *adjectives* (could be none)\n",
    "* A single *noun*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_ner(sentence, pos):\n",
    "    \"\"\"Given a sentence as a list of tokens and it's part of speech tags\n",
    "    this returns a list of the same length with True wherever it thinks\n",
    "    there is a name.\"\"\"\n",
    "    \n",
    "    ret = [False] * len(sentence)\n",
    "    guess_lab = numpy.zeros(len(sentence))\n",
    "    pos_list =''.join(pos)\n",
    "\n",
    "    for p in re.finditer('D?',pos_list):\n",
    "        if p.start()<p.end():\n",
    "            guess_lab[p.start()] = guess_lab[p.start()] + 1\n",
    "\n",
    "            \n",
    "    for p in re.finditer('J*',pos_list):\n",
    "        if p.start()<p.end():\n",
    "            guess_lab[p.start()] = guess_lab[p.start()] + 1\n",
    "\n",
    "    \n",
    "    for p in re.finditer('N{1}',pos_list):\n",
    "        if p.start()<p.end():\n",
    "            guess_lab[p.start()] = guess_lab[p.start()] + 1\n",
    "\n",
    "    for i in range (len(sentence)):\n",
    "        if guess_lab[i] == 3:\n",
    "            ret[i] =True \n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%\n",
      "Percentage correct = 84.7%\n",
      "  (took 1.35122 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Code to test the performance of the NER tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100 \n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = sentence_ner(gmb[i], [pos_to_rpos[p] for p in gmb.pos(i)])\n",
    "    truth = [ner!='O' for ner in gmb.ner(i)]\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==t:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
